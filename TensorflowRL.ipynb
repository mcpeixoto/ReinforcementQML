{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kWo-GNlwpT6"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5w2rucWZwpUA"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0iA0nubwpUX"
   },
   "source": [
    "<img src=\"./images/gym_CartPole.gif\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAO1TBxqwpUX"
   },
   "source": [
    "## 3. Deep Q-learning with PQC Q-function approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uEimdpHwpUY"
   },
   "source": [
    "In this section, you will move to the implementation of the deep Q-learning algorithm presented in <a href=\"https://arxiv.org/abs/2103.15084\" class=\"external\">[2]</a>. As opposed to a policy-gradient approach, the deep Q-learning method uses a PQC to approximate the Q-function of the agent. That is, the PQC defines a function approximator:\n",
    "$$ Q_\\theta(s,a) = \\langle O_a \\rangle_{s,\\theta} $$\n",
    "where $\\langle O_a \\rangle_{s,\\theta}$ are expectation values of observables $O_a$ (one per action) measured at the ouput of the PQC.\n",
    "\n",
    "These Q-values are updated using a loss function derived from Q-learning:\n",
    "$$ \\mathcal{L}(\\theta) = \\frac{1}{|\\mathcal{B}|}\\sum_{s,a,r,s' \\in \\mathcal{B}} \\left(Q_\\theta(s,a) - [r +\\max_{a'} Q_{\\theta'}(s',a')]\\right)^2$$\n",
    "for a batch $\\mathcal{B}$ of $1$-step interactions $(s,a,r,s')$ with the environment, sampled from the replay memory, and parameters $\\theta'$ specifying the target PQC (i.e., a copy of the main PQC, whose parameters are sporadically copied from the main PQC throughout learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTyRzuDYwpUY"
   },
   "source": [
    "You can adopt the same observables used in <a href=\"https://arxiv.org/abs/2103.15084\" class=\"external\">[2]</a> for CartPole, namely a $Z_0Z_1$ Pauli product for action $0$ and a $Z_2Z_3$ Pauli product for action $1$. Both observables are re-scaled so their expectation values are in $[0,1]$ and weighted by an action-specific weight. To implement the re-scaling and weighting of the Pauli products, you can define again an extra `tf.keras.layers.Layer` that stores the action-specific weights and applies them multiplicatively on the expectation values $\\left(1+\\langle Z_0Z_1 \\rangle_{s,\\theta}\\right)/2$ and $\\left(1+\\langle Z_2Z_3 \\rangle_{s,\\theta}\\right)/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 17:23:59.669606: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:23:59.669625: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-26 17:24:01.706834: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.706872: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.706897: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.706921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.706945: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.706977: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.707032: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.707055: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-04-26 17:24:01.707061: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-04-26 17:24:01.707251: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Update package resources to account for version changes.\n",
    "import importlib, pkg_resources\n",
    "importlib.reload(pkg_resources)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_quantum as tfq\n",
    "\n",
    "import gym, cirq, sympy\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import deque, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from cirq.contrib.svg import SVGCircuit\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MX5l96qywpUY"
   },
   "outputs": [],
   "source": [
    "class Rescaling(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Rescaling, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=tf.ones(shape=(1,input_dim)), dtype=\"float32\",\n",
    "            trainable=True, name=\"obs-weights\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.multiply((inputs+1)/2, tf.repeat(self.w,repeats=tf.shape(inputs)[0],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReUploadingPQC(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Performs the transformation (s_1, ..., s_d) -> (theta_1, ..., theta_N, lmbd[1][1]s_1, ..., lmbd[1][M]s_1,\n",
    "        ......., lmbd[d][1]s_d, ..., lmbd[d][M]s_d) for d=input_dim, N=theta_dim and M=n_layers.\n",
    "    An activation function from tf.keras.activations, specified by `activation` ('linear' by default) is\n",
    "        then applied to all lmbd[i][j]s_i.\n",
    "    All angles are finally permuted to follow the alphabetical order of their symbol names, as processed\n",
    "        by the ControlledPQC.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qubits, n_layers, observables, activation=\"linear\", name=\"re-uploading_PQC\"):\n",
    "        super(ReUploadingPQC, self).__init__(name=name)\n",
    "        self.n_layers = n_layers\n",
    "        self.n_qubits = len(qubits)\n",
    "\n",
    "        circuit, theta_symbols, input_symbols = generate_circuit(qubits, n_layers)\n",
    "\n",
    "        theta_init = tf.random_uniform_initializer(minval=0.0, maxval=np.pi)\n",
    "        self.theta = tf.Variable(\n",
    "            initial_value=theta_init(shape=(1, len(theta_symbols)), dtype=\"float32\"),\n",
    "            trainable=True, name=\"thetas\"\n",
    "        )\n",
    "\n",
    "        lmbd_init = tf.ones(shape=(self.n_qubits * self.n_layers,))\n",
    "        self.lmbd = tf.Variable(\n",
    "            initial_value=lmbd_init, dtype=\"float32\", trainable=True, name=\"lambdas\"\n",
    "        )\n",
    "\n",
    "        # Define explicit symbol order.\n",
    "        symbols = [str(symb) for symb in theta_symbols + input_symbols]\n",
    "        self.indices = tf.constant([symbols.index(a) for a in sorted(symbols)])\n",
    "\n",
    "        self.activation = activation\n",
    "        self.empty_circuit = tfq.convert_to_tensor([cirq.Circuit()])\n",
    "        self.computation_layer = tfq.layers.ControlledPQC(circuit, observables)        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs[0] = encoding data for the state.\n",
    "        batch_dim = tf.gather(tf.shape(inputs[0]), 0)\n",
    "        tiled_up_circuits = tf.repeat(self.empty_circuit, repeats=batch_dim)\n",
    "        tiled_up_thetas = tf.tile(self.theta, multiples=[batch_dim, 1])\n",
    "        tiled_up_inputs = tf.tile(inputs[0], multiples=[1, self.n_layers])\n",
    "        scaled_inputs = tf.einsum(\"i,ji->ji\", self.lmbd, tiled_up_inputs)\n",
    "        squashed_inputs = tf.keras.layers.Activation(self.activation)(scaled_inputs)\n",
    "\n",
    "        joined_vars = tf.concat([tiled_up_thetas, squashed_inputs], axis=1)\n",
    "        joined_vars = tf.gather(joined_vars, self.indices, axis=1)\n",
    "\n",
    "        return self.computation_layer([tiled_up_circuits, joined_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_qubit_rotation(qubit, symbols):\n",
    "    \"\"\"\n",
    "    Returns Cirq gates that apply a rotation of the bloch sphere about the X,\n",
    "    Y and Z axis, specified by the values in `symbols`.\n",
    "    \"\"\"\n",
    "    return [cirq.rx(symbols[0])(qubit),\n",
    "            cirq.ry(symbols[1])(qubit),\n",
    "            cirq.rz(symbols[2])(qubit)]\n",
    "\n",
    "def entangling_layer(qubits):\n",
    "    \"\"\"\n",
    "    Returns a layer of CZ entangling gates on `qubits` (arranged in a circular topology).\n",
    "    \"\"\"\n",
    "    cz_ops = [cirq.CZ(q0, q1) for q0, q1 in zip(qubits, qubits[1:])]\n",
    "    cz_ops += ([cirq.CZ(qubits[0], qubits[-1])] if len(qubits) != 2 else [])\n",
    "    return cz_ops\n",
    "\n",
    "def generate_circuit(qubits, n_layers):\n",
    "    \"\"\"Prepares a data re-uploading circuit on `qubits` with `n_layers` layers.\"\"\"\n",
    "    # Number of qubits\n",
    "    n_qubits = len(qubits)\n",
    "\n",
    "    # Sympy symbols for variational angles\n",
    "    params = sympy.symbols(f'theta(0:{3*(n_layers+1)*n_qubits})')\n",
    "    params = np.asarray(params).reshape((n_layers + 1, n_qubits, 3))\n",
    "\n",
    "    # Sympy symbols for encoding angles\n",
    "    inputs = sympy.symbols(f'x(0:{n_layers})'+f'_(0:{n_qubits})')\n",
    "    inputs = np.asarray(inputs).reshape((n_layers, n_qubits))\n",
    "\n",
    "    # Define circuit\n",
    "    circuit = cirq.Circuit()\n",
    "    for l in range(n_layers):\n",
    "        # Variational layer\n",
    "        circuit += cirq.Circuit(one_qubit_rotation(q, params[l, i]) for i, q in enumerate(qubits))\n",
    "        circuit += entangling_layer(qubits)\n",
    "        # Encoding layer\n",
    "        circuit += cirq.Circuit(cirq.rx(inputs[l, i])(q) for i, q in enumerate(qubits))\n",
    "\n",
    "    # Last varitional layer\n",
    "    circuit += cirq.Circuit(one_qubit_rotation(q, params[n_layers, i]) for i,q in enumerate(qubits))\n",
    "\n",
    "    return circuit, list(params.flat), list(inputs.flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oesnEQa7wpUY"
   },
   "source": [
    "Prepare the definition of your PQC and its observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cpV0PxZqwpUY"
   },
   "outputs": [],
   "source": [
    "n_qubits = 4 # Dimension of the state vectors in CartPole\n",
    "n_layers = 5 # Number of layers in the PQC\n",
    "n_actions = 2 # Number of actions in CartPole\n",
    "\n",
    "qubits = cirq.GridQubit.rect(1, n_qubits)\n",
    "ops = [cirq.Z(q) for q in qubits]\n",
    "observables = [ops[0]*ops[1], ops[2]*ops[3]] # Z_0*Z_1 for action 0 and Z_2*Z_3 for action 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLMvQBXFwpUZ"
   },
   "source": [
    "Define a `tf.keras.Model` that, similarly to the PQC-policy model, constructs a Q-function approximator that is used to generate the main and target models of our Q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PBGM6RHIwpUZ"
   },
   "outputs": [],
   "source": [
    "def generate_model_Qlearning(qubits, n_layers, n_actions, observables, target):\n",
    "    \"\"\"Generates a Keras model for a data re-uploading PQC Q-function approximator.\"\"\"\n",
    "\n",
    "    input_tensor = tf.keras.Input(shape=(len(qubits), ), dtype=tf.dtypes.float32, name='input')\n",
    "    re_uploading_pqc = ReUploadingPQC(qubits, n_layers, observables, activation='tanh')([input_tensor])\n",
    "    process = tf.keras.Sequential([Rescaling(len(observables))], name=target*\"Target\"+\"Q-values\")\n",
    "    Q_values = process(re_uploading_pqc)\n",
    "    model = tf.keras.Model(inputs=[input_tensor], outputs=Q_values)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = generate_model_Qlearning(qubits, n_layers, n_actions, observables, False)\n",
    "model_target = generate_model_Qlearning(qubits, n_layers, n_actions, observables, True)\n",
    "\n",
    "model_target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "57TxgIN5wpUZ",
    "outputId": "da6a3aee-1722-40b3-fae5-2972d8c2ed88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "jHp42R4twpUa",
    "outputId": "69a20e50-fde4-4634-897b-01edaeab420a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model_target, show_shapes=True, dpi=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vricJOvXwpUa"
   },
   "source": [
    "You can now implement the deep Q-learning algorithm and test it on the CartPole-v1 environment. For the policy of the agent, you can use an $\\varepsilon$-greedy policy:\n",
    "$$ \\pi(a|s) =\n",
    "\\begin{cases}\n",
    "\\delta_{a,\\text{argmax}_{a'} Q_\\theta(s,a')}\\quad \\text{w.p.}\\quad 1 - \\varepsilon\\\\\n",
    "\\frac{1}{\\text{num_actions}}\\quad \\quad \\quad \\quad \\text{w.p.}\\quad \\varepsilon\n",
    "\\end{cases} $$\n",
    "where $\\varepsilon$ is multiplicatively decayed at each episode of interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMteuedswpUb"
   },
   "source": [
    "Start by defining a function that performs an interaction step in the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0L9cV26PwpUb"
   },
   "outputs": [],
   "source": [
    "def interact_env(state, model, epsilon, n_actions, env):\n",
    "    # Preprocess state\n",
    "    state_array = np.array(state) \n",
    "    state = tf.convert_to_tensor([state_array])\n",
    "\n",
    "    # Sample action\n",
    "    coin = np.random.random()\n",
    "    if coin > epsilon:\n",
    "        q_vals = model([state])\n",
    "        action = int(tf.argmax(q_vals[0]).numpy())\n",
    "    else:\n",
    "        action = np.random.choice(n_actions)\n",
    "\n",
    "    # Apply sampled action in the environment, receive reward and next state\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    interaction = {'state': state_array, 'action': action, 'next_state': next_state.copy(),\n",
    "                   'reward': reward, 'done':np.float32(done)}\n",
    "    \n",
    "    return interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDiw3iJywpUb"
   },
   "source": [
    "and a function that updates the Q-function using a batch of interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RR2DjesVwpUb"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def Q_learning_update(states, actions, rewards, next_states, done, model, gamma, n_actions):\n",
    "    states = tf.convert_to_tensor(states)\n",
    "    actions = tf.convert_to_tensor(actions)\n",
    "    rewards = tf.convert_to_tensor(rewards)\n",
    "    next_states = tf.convert_to_tensor(next_states)\n",
    "    done = tf.convert_to_tensor(done)\n",
    "\n",
    "    # Compute their target q_values and the masks on sampled actions\n",
    "    future_rewards = model_target([next_states])\n",
    "    target_q_values = rewards + (gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "                                                   * (1.0 - done))\n",
    "    masks = tf.one_hot(actions, n_actions)\n",
    "\n",
    "    # Train the model on the states and target Q-values\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_variables)\n",
    "        q_values = model([states])\n",
    "        q_values_masked = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "        loss = tf.keras.losses.Huber()(target_q_values, q_values_masked)\n",
    "\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    for optimizer, w in zip([optimizer_in, optimizer_var, optimizer_out], [w_in, w_var, w_out]):\n",
    "        optimizer.apply_gradients([(grads[w], model.trainable_variables[w])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfXHhqaPwpUb"
   },
   "source": [
    "Define the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SQ937aYPwpUc"
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "n_episodes = 2000\n",
    "\n",
    "# Define replay memory\n",
    "max_memory_length = 10000 # Maximum replay length\n",
    "replay_memory = deque(maxlen=max_memory_length)\n",
    "\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "decay_epsilon = 0.99 # Decay rate of epsilon greedy parameter\n",
    "batch_size = 16\n",
    "steps_per_update = 10 # Train the model every x steps\n",
    "steps_per_target_update = 30 # Update the target model every x steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHsHnuHmwpUc"
   },
   "source": [
    "Prepare the optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "713nl3oUwpUc"
   },
   "outputs": [],
   "source": [
    "optimizer_in = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_var = tf.keras.optimizers.Adam(learning_rate=0.001, amsgrad=True)\n",
    "optimizer_out = tf.keras.optimizers.Adam(learning_rate=0.1, amsgrad=True)\n",
    "\n",
    "# Assign the model parameters to each optimizer\n",
    "w_in, w_var, w_out = 1, 0, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwE0buDowpUd"
   },
   "source": [
    "Now implement the main training loop of the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjjTamvywpUd"
   },
   "source": [
    "Note: This agent may need to simulate several million quantum circuits and can take as much as ~40 minutes to finish training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "er9fXHH_wpUd",
    "outputId": "b165066b-c239-4bf0-b444-7c710640241c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/2000, average last 10 rewards 21.8\n",
      "Episode 20/2000, average last 10 rewards 23.9\n",
      "Episode 30/2000, average last 10 rewards 19.9\n",
      "Episode 40/2000, average last 10 rewards 23.3\n",
      "Episode 50/2000, average last 10 rewards 17.6\n",
      "Episode 60/2000, average last 10 rewards 22.4\n",
      "Episode 70/2000, average last 10 rewards 26.2\n",
      "Episode 80/2000, average last 10 rewards 23.1\n",
      "Episode 90/2000, average last 10 rewards 56.4\n",
      "Episode 100/2000, average last 10 rewards 39.4\n",
      "Episode 110/2000, average last 10 rewards 57.4\n",
      "Episode 120/2000, average last 10 rewards 50.7\n",
      "Episode 130/2000, average last 10 rewards 51.9\n",
      "Episode 140/2000, average last 10 rewards 51.6\n",
      "Episode 150/2000, average last 10 rewards 53.3\n",
      "Episode 160/2000, average last 10 rewards 41.7\n",
      "Episode 170/2000, average last 10 rewards 49.1\n",
      "Episode 180/2000, average last 10 rewards 65.6\n",
      "Episode 190/2000, average last 10 rewards 62.9\n",
      "Episode 200/2000, average last 10 rewards 50.0\n",
      "Episode 210/2000, average last 10 rewards 34.7\n",
      "Episode 220/2000, average last 10 rewards 48.8\n",
      "Episode 230/2000, average last 10 rewards 60.1\n",
      "Episode 240/2000, average last 10 rewards 57.0\n",
      "Episode 250/2000, average last 10 rewards 66.5\n",
      "Episode 260/2000, average last 10 rewards 57.2\n",
      "Episode 270/2000, average last 10 rewards 64.9\n",
      "Episode 280/2000, average last 10 rewards 64.8\n",
      "Episode 290/2000, average last 10 rewards 69.0\n",
      "Episode 300/2000, average last 10 rewards 62.3\n",
      "Episode 310/2000, average last 10 rewards 42.2\n",
      "Episode 320/2000, average last 10 rewards 62.4\n",
      "Episode 330/2000, average last 10 rewards 38.7\n",
      "Episode 340/2000, average last 10 rewards 60.4\n",
      "Episode 350/2000, average last 10 rewards 63.2\n",
      "Episode 360/2000, average last 10 rewards 47.9\n",
      "Episode 370/2000, average last 10 rewards 37.6\n",
      "Episode 380/2000, average last 10 rewards 40.5\n",
      "Episode 390/2000, average last 10 rewards 40.5\n",
      "Episode 400/2000, average last 10 rewards 53.0\n",
      "Episode 410/2000, average last 10 rewards 26.1\n",
      "Episode 420/2000, average last 10 rewards 26.4\n",
      "Episode 430/2000, average last 10 rewards 12.2\n",
      "Episode 440/2000, average last 10 rewards 24.1\n",
      "Episode 450/2000, average last 10 rewards 42.8\n",
      "Episode 460/2000, average last 10 rewards 54.8\n",
      "Episode 470/2000, average last 10 rewards 47.1\n",
      "Episode 480/2000, average last 10 rewards 31.3\n",
      "Episode 490/2000, average last 10 rewards 115.0\n",
      "Episode 500/2000, average last 10 rewards 95.1\n",
      "Episode 510/2000, average last 10 rewards 183.8\n",
      "Episode 520/2000, average last 10 rewards 201.9\n",
      "Episode 530/2000, average last 10 rewards 97.3\n",
      "Episode 540/2000, average last 10 rewards 109.3\n",
      "Episode 550/2000, average last 10 rewards 156.5\n",
      "Episode 560/2000, average last 10 rewards 108.5\n",
      "Episode 570/2000, average last 10 rewards 147.4\n",
      "Episode 580/2000, average last 10 rewards 132.6\n",
      "Episode 590/2000, average last 10 rewards 138.2\n",
      "Episode 600/2000, average last 10 rewards 167.4\n",
      "Episode 610/2000, average last 10 rewards 65.9\n",
      "Episode 620/2000, average last 10 rewards 164.9\n",
      "Episode 630/2000, average last 10 rewards 134.3\n",
      "Episode 640/2000, average last 10 rewards 174.6\n",
      "Episode 650/2000, average last 10 rewards 164.2\n",
      "Episode 660/2000, average last 10 rewards 205.8\n",
      "Episode 670/2000, average last 10 rewards 181.1\n",
      "Episode 680/2000, average last 10 rewards 149.9\n",
      "Episode 690/2000, average last 10 rewards 57.4\n",
      "Episode 700/2000, average last 10 rewards 193.1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "    \n",
    "episode_reward_history = []\n",
    "step_count = 0\n",
    "for episode in range(n_episodes):\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        # Interact with env\n",
    "        interaction = interact_env(state, model, epsilon, n_actions, env)\n",
    "        \n",
    "        # Store interaction in the replay memory\n",
    "        replay_memory.append(interaction)\n",
    "        \n",
    "        state = interaction['next_state']\n",
    "        episode_reward += interaction['reward']\n",
    "        step_count += 1\n",
    "        \n",
    "        # Update model\n",
    "        if step_count % steps_per_update == 0:\n",
    "            # Sample a batch of interactions and update Q_function\n",
    "            training_batch = np.random.choice(replay_memory, size=batch_size)\n",
    "            Q_learning_update(np.asarray([x['state'] for x in training_batch]),\n",
    "                              np.asarray([x['action'] for x in training_batch]),\n",
    "                              np.asarray([x['reward'] for x in training_batch], dtype=np.float32),\n",
    "                              np.asarray([x['next_state'] for x in training_batch]),\n",
    "                              np.asarray([x['done'] for x in training_batch], dtype=np.float32),\n",
    "                              model, gamma, n_actions)\n",
    "        \n",
    "        # Update target model\n",
    "        if step_count % steps_per_target_update == 0:\n",
    "            model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        # Check if the episode is finished\n",
    "        if interaction['done']:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon * decay_epsilon, epsilon_min)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if (episode+1)%10 == 0:\n",
    "        avg_rewards = np.mean(episode_reward_history[-10:])\n",
    "        print(\"Episode {}/{}, average last 10 rewards {}\".format(\n",
    "            episode+1, n_episodes, avg_rewards))\n",
    "        if avg_rewards >= 500.0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG8BWOSYwpUd"
   },
   "source": [
    "Plot the learning history of the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "sSRMtk-swpUe",
    "outputId": "a2a4c5a8-92cf-495a-da9d-6be0e8f673a2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(episode_reward_history)\n",
    "plt.xlabel('Epsiode')\n",
    "plt.ylabel('Collected rewards')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_7rJf0iwpUe"
   },
   "source": [
    "Similarly to the plot above, you should see that after ~1000 episodes, the performance of the agent gets close to optimal, i.e., 500 rewards per episode. Learning takes longer for Q-learning agents since the Q-function is a \"richer\" function to be learned than the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8X49f8owpUe"
   },
   "source": [
    "## 4. Exercise\n",
    "\n",
    "Now that you have trained two different types of models, try experimenting with different environments (and different numbers of qubits and layers). You could also try combining the PQC models of the last two sections into an [actor-critic agent](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#actor-critic)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jxWGru_NwpUK",
    "_u3QBKbvwpUP",
    "X8X49f8owpUe"
   ],
   "name": "quantum_reinforcement_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
